{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sentiment Analysis of Reviews using RNNs in TensorFlow\n",
    "\n",
    "Modified from original code here: https://github.com/adeshpande3/LSTM-Sentiment-Analysis/blob/master/Oriole%20LSTM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Some imports to make code compatible with Python 2 as well as 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "2.0.2\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(mp.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, unzip and untar files in an automated way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DOWNLOADED_FILENAME = 'ImdbReviews.tar.gz'\n",
    "\n",
    "def download_file(url_path):\n",
    "    if not os.path.exists(DOWNLOADED_FILENAME):\n",
    "        filename, _ = urllib.request.urlretrieve(url_path, DOWNLOADED_FILENAME)\n",
    "\n",
    "    print('Found and verified file from this path: ', url_path)\n",
    "    print('Downloaded file: ', DOWNLOADED_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract reviews and the corresponding positive and negative labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TOKEN_REGEX = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "\n",
    "def get_reviews(dirname, positive=True):\n",
    "    label = 1 if positive else 0\n",
    "\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(dirname):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(dirname + filename, 'r+') as f:\n",
    "                review = f.read().decode('utf-8')\n",
    "                review = review.lower().replace(\"<br />\", \" \")\n",
    "                review = re.sub(TOKEN_REGEX, '', review)\n",
    "                \n",
    "                reviews.append(review)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return reviews, labels           \n",
    "\n",
    "def extract_labels_data():\n",
    "    # If the file has not already been extracted\n",
    "    if not os.path.exists('aclImdb'):\n",
    "        with tarfile.open(DOWNLOADED_FILENAME) as tar:\n",
    "            tar.extractall()\n",
    "            tar.close()\n",
    "        \n",
    "    positive_reviews, positive_labels = get_reviews(\"aclImdb/train/pos/\", positive=True)\n",
    "    negative_reviews, negative_labels = get_reviews(\"aclImdb/train/neg/\", positive=False)\n",
    "\n",
    "    data = positive_reviews + negative_reviews\n",
    "    labels = positive_labels + negative_labels\n",
    "\n",
    "    return labels, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified file from this path:  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Downloaded file:  ImdbReviews.tar.gz\n"
     ]
    }
   ],
   "source": [
    "URL_PATH = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "download_file(URL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels, data = extract_labels_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt',\n",
       " u'homelessness or houselessness as george carlin stated has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school work or vote for the matter most people think of the homeless as just a lost cause while worrying about things such as racism the war on iraq pressuring kids to succeed technology the elections inflation or worrying if theyll be next to end up on the streets  but what if you were given a bet to live on the streets for a month without the luxuries you once had from a home the entertainment sets a bathroom pictures on the wall a computer and everything you once treasure to see what its like to be homeless that is goddard bolts lesson  mel brooks who directs who stars as bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival jeffery tambor to see if he can live in the streets for thirty days without the luxuries if bolt succeeds he can do what he wants with a future project of making more buildings the bets on where bolt is thrown on the street with a bracelet on his leg to monitor his every move where he cant step off the sidewalk hes given the nickname pepto by a vagrant after its written on his forehead where bolt meets other characters including a woman by the name of molly lesley ann warren an exdancer who got divorce before losing her home and her pals sailor howard morris and fumes teddy wilson who are already used to the streets theyre survivors bolt isnt hes not used to reaching mutual agreements like he once did when being rich where its fight or flight kill or be killed  while the love connection between molly and bolt wasnt necessary to plot i found life stinks to be one of mel brooks observant films where prior to being a comedy it shows a tender side compared to his slapstick work such as blazing saddles young frankenstein or spaceballs for the matter to show what its like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they dont know what to do with their money maybe they should give it to the homeless instead of using it like monopoly money  or maybe this film will inspire you to help others',\n",
       " u'brilliant overacting by lesley ann warren best dramatic hobo lady i have ever seen and love scenes in clothes warehouse are second to none the corn on face is a classic as good as anything in blazing saddles the take on lawyers is also superb after being accused of being a turncoat selling out his boss and being dishonest the lawyer of pepto bolt shrugs indifferently im a lawyer he says three funny words jeffrey tambor a favorite from the later larry sanders show is fantastic here too as a mad millionaire who wants to crush the ghetto his character is more malevolent than usual the hospital scene and the scene where the homeless invade a demolition site are alltime classics look for the legs scene and the two big diggers fighting one bleeds this movie gets better each time i see it which is quite often',\n",
       " u'this is easily the most underrated film inn the brooks cannon sure its flawed it does not give a realistic view of homelessness unlike say how citizen kane gave a realistic view of lounge singers or titanic gave a realistic view of italians you idiots many of the jokes fall flat but still this film is very lovable in a way many comedies are not and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive its not the fisher king but its not crap either my only complaint is that brooks should have cast someone else in the lead i love mel as a director and writer not so much as a lead',\n",
       " u'this is not the typical mel brooks film it was much less slapstick than most of his movies and actually had a plot that was followable leslie ann warren made the movie she is such a fantastic underrated actress there were some moments that could have been fleshed out a bit more and some scenes that could probably have been cut to make the room to do so but all in all this is worth the price to rent and see it the acting was good overall brooks himself did a good job without his characteristic speaking to directly to the audience again warren was the best actor in the movie but fume and sailor both played their parts well']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470\n"
     ]
    }
   ],
   "source": [
    "max_document_length = max([len(x.split(\" \")) for x in data])\n",
    "print(max_document_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many words to consider in each review?\n",
    "\n",
    "Majority of the reviews fall under 250 words. This a number we've chosen based on some analysis of the data:\n",
    "\n",
    "* Count the number of words in each file and divide by number of files to get an average i.e. **avg_words_per_file = total_words / num_files**\n",
    "* Plot the words per file on matplot lib and try find a number which includes a majority of files\n",
    "\n",
    "Word embeddings all have the same dimensionality which you can specify. A document is a vector of word embeddings (one dbpedia instance is a document in this case)\n",
    "\n",
    "* Each document should be of the **same length**, documents longer than the MAX_SEQUENCE_LENGTH are truncated to this length\n",
    "* The other documents will be **padded** by a special symbol to be the same max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary processor\n",
    " \n",
    "http://tflearn.org/data_utils/\n",
    " \n",
    "Library to map every word which occurs in our dataset to a unique identifer. If there are 10023 words each will be assigned a unique id from 1-10023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform every word to a representation using unique ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111526\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array(list(vocab_processor.fit_transform(data)))\n",
    "y_output = np.array(labels)\n",
    "\n",
    "vocabulary_size = len(vocab_processor.vocabulary_)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'this is easily the most underrated film inn the brooks cannon sure its flawed it does not give a realistic view of homelessness unlike say how citizen kane gave a realistic view of lounge singers or titanic gave a realistic view of italians you idiots many of the jokes fall flat but still this film is very lovable in a way many comedies are not and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive its not the fisher king but its not crap either my only complaint is that brooks should have cast someone else in the lead i love mel as a director and writer not so much as a lead',\n",
       " u'this is not the typical mel brooks film it was much less slapstick than most of his movies and actually had a plot that was followable leslie ann warren made the movie she is such a fantastic underrated actress there were some moments that could have been fleshed out a bit more and some scenes that could probably have been cut to make the room to do so but all in all this is worth the price to rent and see it the acting was good overall brooks himself did a good job without his characteristic speaking to directly to the audience again warren was the best actor in the movie but fume and sailor both played their parts well']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[290,   3, 364,  10, 121, 365, 291, 366,  10, 168, 367, 368, 162,\n",
       "        369,   7, 370, 243, 286,   4, 371, 372,  53,  92, 373, 374, 375,\n",
       "        376, 377, 378,   4, 371, 372,  53, 379, 380,  93, 381, 378,   4,\n",
       "        371, 372,  53, 382, 146, 383,  83,  53,  10, 384, 385, 386, 103,\n",
       "        387, 290, 291,   3, 388, 389,  25,   4, 390,  83, 391, 238, 243,\n",
       "         61,  30, 392,  32, 206,  25,   4, 393,  17,  14,  53,  10, 121,\n",
       "        394, 395, 396,  53, 397,   3, 398, 399, 162, 243,  10, 400, 401,\n",
       "        103, 162, 243, 402, 403,  22, 404, 405,   3,  32, 168, 285, 301,\n",
       "        406, 407, 408,  25,  10,  28,  59, 252, 167,  13,   4, 409,  61,\n",
       "        410, 243, 411,  35,  13,   4,  28,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [290,   3, 243,  10, 412, 167, 168, 291,   7, 413,  35, 414, 267,\n",
       "         38, 121,  53, 199, 415,  61, 416, 153,   4, 257,  32, 413, 417,\n",
       "        418, 223, 224, 419,  10, 358, 420,   3,  20,   4, 335, 365, 421,\n",
       "        422, 110,  14, 423,  32, 424, 301,  99, 425, 320,   4, 426, 193,\n",
       "         61,  14, 304,  32, 424, 427, 301,  99, 428,  30, 178,  10, 429,\n",
       "         30, 188, 411, 103,  56,  25,  56, 290,   3, 430,  10, 431,  30,\n",
       "        432,  61,  46,   7,  10, 433, 413, 311, 434, 168, 435, 114,   4,\n",
       "        311, 436, 151, 199, 437, 438,  30, 439,  30,  10, 440, 441, 224,\n",
       "        413,  10, 297, 442,  25,  10, 358, 103, 443,  61, 232, 444, 445,\n",
       "         49, 446, 447,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  13,  21,  22,  23,  24,  25,\n",
       "         10,  26,  27,  28,  29,  30,  31,  32,   1,  33,  34,   3,  35,\n",
       "         36,  30,  37,  38,   3,  21,  10,  39,  30,  40,  41,  10,  42,\n",
       "         43,  44,  45,  46,  47,  48,  49,  50,  21,  51,  10,  52,  53,\n",
       "         10,  54,  55,  56,  57,  29,  53,  10,  58,  59,  60,  61,  49,\n",
       "         43,  62,  59,  63,  10,  64,  25,  65,   4,  66,  67,  68,  30,\n",
       "         69,  70,  10,  18,  59,  71,  72,   9,   2,   4,  73,  74,  75,\n",
       "         76,  77,  30,  78,  79,  53,  80,  21,  66,  81,  30,   1,   2,\n",
       "         59,  82,  32,  83,  84,  53,  22,  85,  86,  32,   1,   2,   3,\n",
       "         87,  88,  89,   4,  90,  32,   7,  91,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [ 92,  93,  94,  13,  95,  96,  97,  98,  99, 100, 101, 102,  24,\n",
       "        103, 104,   4, 105,  30, 106, 107, 108,  10, 109,  32, 110, 111,\n",
       "        112, 113,  44, 114, 115, 116, 117,  30,  18, 118,  93, 119, 102,\n",
       "         10, 120, 121, 122,  86,  53,  10, 123,  13, 124,   4, 125, 126,\n",
       "        127, 128,  17, 129,  20,  13, 130,  10, 131, 108, 132, 133, 134,\n",
       "         30, 135, 136,  10, 137, 138,  93, 128, 139, 140, 141, 142,  30,\n",
       "        143, 144, 108,  10, 145, 103,  89, 139, 146, 110, 147,   4, 148,\n",
       "         30, 149, 108,  10, 145, 102,   4, 150, 151,  10, 152, 146, 111,\n",
       "        153, 116,   4, 154,  10, 155, 156,   4, 157, 158, 108,  10, 159,\n",
       "          4, 160,  61, 115, 146, 111, 161,  30,  46,  89, 162, 163,  30,\n",
       "        141, 123,  32,   3, 164, 165, 166, 167, 168,  44, 169,  44, 170,\n",
       "         13, 171, 172,   4, 173, 174,  44,  98, 115,  25,  10, 175, 176,\n",
       "        177,  30, 178,   4, 148, 179,   4, 180, 181, 182, 183,  30,  46,\n",
       "        139, 184,  45, 149,  25,  10, 145, 102, 185, 186, 151,  10, 152,\n",
       "        139, 171, 187, 184,  45, 188,  89, 184, 189, 179,   4, 190, 191,\n",
       "         53, 192, 193, 194,  10, 195, 108, 196, 171,   3, 197, 108,  10,\n",
       "        109, 179,   4, 198, 108, 199, 200,  30, 201, 199, 202, 203, 196,\n",
       "        184, 204, 205, 206,  10, 207, 208, 147,  10, 209, 210, 211,   4,\n",
       "        212, 213, 162, 214, 108, 199, 215, 196, 171, 216,  15, 217, 218,\n",
       "          4, 219, 211]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the data so the training instances are randomly fed to the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(22)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(x_data)))\n",
    "\n",
    "x_shuffled = x_data[shuffle_indices]\n",
    "y_shuffled = y_output[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = 5000\n",
    "TOTAL_DATA = 6000\n",
    "\n",
    "train_data = x_shuffled[:TRAIN_DATA]\n",
    "train_target = y_shuffled[:TRAIN_DATA]\n",
    "\n",
    "test_data = x_shuffled[TRAIN_DATA:TOTAL_DATA]\n",
    "test_target = y_shuffled[TRAIN_DATA:TOTAL_DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, MAX_SEQUENCE_LENGTH])\n",
    "y = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 25\n",
    "embedding_size = 50\n",
    "max_label = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings to represent words\n",
    "\n",
    "These embeddings are generated as a part of the training process of the RNN. The embeddings are trained using the reviews in the training dataset.\n",
    "\n",
    "* *embedding_matrix* This is a matrix which holds the embeddings for every word in the vocabulary. The values are determined during the training process\n",
    "* *embeddings* The embeddings for the words which are input as a part of one training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embeddings = tf.nn.embedding_lookup(embedding_matrix, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(111526, 50) dtype=float32_ref>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(?, 250, 50) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(embedding_size)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from an RNN of LSTM cells\n",
    "\n",
    "(ouput, (**final_state**, other_state_info))\n",
    "\n",
    "We're interested in the final state of this RNN because those are the encodings we feed into the prediction layer of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_, (encoding, _) = tf.nn.dynamic_rnn(lstmCell, embeddings, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn_1/while/Exit_2:0' shape=(?, 50) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A densely connected prediction layer\n",
    "\n",
    "* *activation=None* because the activation will be part of the tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "* *cross_entropy* the loss function for probability distributions\n",
    "* *max_label* the number of outputs of the prediction layer, here is 2, positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(encoding, max_label, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the output with the highest probability and compare against the true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.equal(tf.argmax(logits, 1), tf.cast(y, tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test Loss: 0.69, Test Acc: 0.49\n",
      "Epoch: 2, Test Loss: 0.8, Test Acc: 0.505\n",
      "Epoch: 3, Test Loss: 0.83, Test Acc: 0.602\n",
      "Epoch: 4, Test Loss: 0.8, Test Acc: 0.731\n",
      "Epoch: 5, Test Loss: 1.1, Test Acc: 0.759\n",
      "Epoch: 6, Test Loss: 1.3, Test Acc: 0.774\n",
      "Epoch: 7, Test Loss: 1.3, Test Acc: 0.796\n",
      "Epoch: 8, Test Loss: 1.3, Test Acc: 0.797\n",
      "Epoch: 9, Test Loss: 1.4, Test Acc: 0.799\n",
      "Epoch: 10, Test Loss: 1.5, Test Acc: 0.809\n",
      "Epoch: 11, Test Loss: 1.5, Test Acc: 0.813\n",
      "Epoch: 12, Test Loss: 1.5, Test Acc: 0.813\n",
      "Epoch: 13, Test Loss: 1.6, Test Acc: 0.813\n",
      "Epoch: 14, Test Loss: 1.6, Test Acc: 0.814\n",
      "Epoch: 15, Test Loss: 1.6, Test Acc: 0.819\n",
      "Epoch: 16, Test Loss: 1.7, Test Acc: 0.82\n",
      "Epoch: 17, Test Loss: 1.7, Test Acc: 0.82\n",
      "Epoch: 18, Test Loss: 1.8, Test Acc: 0.82\n",
      "Epoch: 19, Test Loss: 1.8, Test Acc: 0.818\n",
      "Epoch: 20, Test Loss: 1.9, Test Acc: 0.819\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        num_batches = int(len(train_data) // batch_size) + 1\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            # Select train data\n",
    "            min_ix = i * batch_size\n",
    "            max_ix = np.min([len(train_data), ((i+1) * batch_size)])\n",
    "\n",
    "            x_train_batch = train_data[min_ix:max_ix]\n",
    "            y_train_batch = train_target[min_ix:max_ix]\n",
    "            \n",
    "            train_dict = {x: x_train_batch, y: y_train_batch}\n",
    "            session.run(train_step, feed_dict=train_dict)\n",
    "            \n",
    "            train_loss, train_acc = session.run([loss, accuracy], feed_dict=train_dict)\n",
    "\n",
    "        test_dict = {x: test_data, y: test_target}\n",
    "        test_loss, test_acc = session.run([loss, accuracy], feed_dict=test_dict)    \n",
    "        print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.5}'.format(epoch + 1, test_loss, test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
